=== NCN Master nodes: ncn-m001 ncn-m002 ncn-m003 ===
=== NCN Worker nodes: ncn-w001 ncn-w002 ncn-w003 ncn-w004 ncn-w005 ===
=== NCN Storage nodes: ncn-s001 ncn-s002 ncn-s003 ===

             +++++ NCN Health Checks +++++
=== Can be executed on any worker or master ncn node. ===
=== Executing on ncn-m001, Wed Nov  6 18:19:10 UTC 2024 ===
=== Active CSM version:


**************************************************************************

=== Check Kubernetes' Master and Worker Node Status. ===
=== Verify Kubernetes' Node "Ready" Status and Version. ===
Wed Nov  6 18:19:11 UTC 2024
NAME       STATUS   ROLES           AGE     VERSION    INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                              KERNEL-VERSION               CONTAINER-RUNTIME
ncn-m001   Ready    control-plane   12d     v1.24.17   10.252.1.12   <none>        SUSE Linux Enterprise Server 15 SP6   6.4.0-150600.23.17-default   containerd://1.7.21
ncn-m002   Ready    control-plane   3d20h   v1.24.17   10.252.1.11   <none>        SUSE Linux Enterprise Server 15 SP6   6.4.0-150600.23.17-default   containerd://1.7.21
ncn-m003   Ready    control-plane   3d20h   v1.24.17   10.252.1.10   <none>        SUSE Linux Enterprise Server 15 SP6   6.4.0-150600.23.17-default   containerd://1.7.21
ncn-w001   Ready    <none>          4d1h    v1.24.17   10.252.1.9    <none>        SUSE Linux Enterprise Server 15 SP6   6.4.0-150600.23.17-default   containerd://1.7.21
ncn-w002   Ready    <none>          4d1h    v1.24.17   10.252.1.8    <none>        SUSE Linux Enterprise Server 15 SP6   6.4.0-150600.23.17-default   containerd://1.7.21
ncn-w003   Ready    <none>          4d      v1.24.17   10.252.1.7    <none>        SUSE Linux Enterprise Server 15 SP6   6.4.0-150600.23.17-default   containerd://1.7.21
ncn-w004   Ready    <none>          4d      v1.24.17   10.252.1.13   <none>        SUSE Linux Enterprise Server 15 SP6   6.4.0-150600.23.17-default   containerd://1.7.21
ncn-w005   Ready    <none>          3d22h   v1.24.17   10.252.1.14   <none>        SUSE Linux Enterprise Server 15 SP6   6.4.0-150600.23.17-default   containerd://1.7.21
 --- PASSED --- 

**************************************************************************

=== Check Ceph Health Status. ===
=== Verify "health: HEALTH_OK" Status. ===
=== At times a status of HEALTH_WARN, too few PGs per OSD, and/or large   omap objects, may be okay. ===
=== date; ssh ncn-m001 ceph -s; ===
Wed Nov  6 18:19:11 UTC 2024
  cluster:
    id:     e1ce52a6-92aa-11ef-ae72-1402ecd97c88
    health: HEALTH_OK
 
  services:
    mon: 3 daemons, quorum ncn-s001,ncn-s002,ncn-s003 (age 3d)
    mgr: ncn-s001.lwgvin(active, since 3d), standbys: ncn-s003.tzpfbg, ncn-s002.rqjssn
    mds: 2/2 daemons up, 3 standby, 1 hot standby
    osd: 24 osds: 24 up (since 3d), 24 in (since 3d)
    rgw: 3 daemons active (3 hosts, 1 zones)
 
  data:
    volumes: 2/2 healthy
    pools:   15 pools, 881 pgs
    objects: 362.31k objects, 692 GiB
    usage:   1.7 TiB used, 40 TiB / 42 TiB avail
    pgs:     881 active+clean
 
  io:
    client:   134 KiB/s rd, 3.8 MiB/s wr, 87 op/s rd, 317 op/s wr
 
 --- PASSED --- 

**************************************************************************

=== Check the Health of the Etcd Clusters in all Namespaces. ===
=== Verify a "healthy" Report for Each Etcd Pod. ===
Wed Nov  6 18:19:13 UTC 2024
### cray-bss-bitnami-etcd-1 ###
127.0.0.1:2379 is healthy: successfully committed proposal: took = 4.437893ms
### cray-bss-bitnami-etcd-2 ###
127.0.0.1:2379 is healthy: successfully committed proposal: took = 3.90872ms
### cray-bss-bitnami-etcd-0 ###
127.0.0.1:2379 is healthy: successfully committed proposal: took = 4.555329ms
### cray-fas-bitnami-etcd-1 ###
127.0.0.1:2379 is healthy: successfully committed proposal: took = 4.805997ms
### cray-fas-bitnami-etcd-2 ###
127.0.0.1:2379 is healthy: successfully committed proposal: took = 5.013702ms
### cray-fas-bitnami-etcd-0 ###
127.0.0.1:2379 is healthy: successfully committed proposal: took = 4.262484ms
### cray-hbtd-bitnami-etcd-1 ###
127.0.0.1:2379 is healthy: successfully committed proposal: took = 5.32413ms
### cray-hbtd-bitnami-etcd-2 ###
127.0.0.1:2379 is healthy: successfully committed proposal: took = 4.232017ms
### cray-hbtd-bitnami-etcd-0 ###
127.0.0.1:2379 is healthy: successfully committed proposal: took = 3.564304ms
### cray-hmnfd-bitnami-etcd-1 ###
127.0.0.1:2379 is healthy: successfully committed proposal: took = 4.221631ms
### cray-hmnfd-bitnami-etcd-2 ###
127.0.0.1:2379 is healthy: successfully committed proposal: took = 3.224331ms
### cray-hmnfd-bitnami-etcd-0 ###
127.0.0.1:2379 is healthy: successfully committed proposal: took = 4.514062ms
### cray-power-control-bitnami-etcd-2 ###
127.0.0.1:2379 is healthy: successfully committed proposal: took = 4.383271ms
### cray-power-control-bitnami-etcd-0 ###
127.0.0.1:2379 is healthy: successfully committed proposal: took = 3.695584ms
### cray-power-control-bitnami-etcd-1 ###
127.0.0.1:2379 is healthy: successfully committed proposal: took = 4.252385ms
 --- PASSED --- 

**************************************************************************

=== Check the Number of Pods in Each Cluster. Verify they are Balanced. ===
=== Each cluster should contain at least three pods, but may contain more. ===
=== Ensure that no two pods in a given cluster exist on the same worker node. ===
Wed Nov  6 18:19:21 UTC 2024
cray-bss-bitnami-etcd-0                                           2/2     Running     0               4d      10.44.0.44    ncn-w003   <none>           <none>
cray-bss-bitnami-etcd-1                                           2/2     Running     0               11h     10.34.0.23    ncn-w002   <none>           <none>
cray-bss-bitnami-etcd-2                                           2/2     Running     0               35h     10.36.0.20    ncn-w001   <none>           <none>

cray-fas-bitnami-etcd-0                                           2/2     Running     0               4d      10.44.0.39    ncn-w003   <none>           <none>
cray-fas-bitnami-etcd-1                                           2/2     Running     0               11h     10.34.0.21    ncn-w002   <none>           <none>
cray-fas-bitnami-etcd-2                                           2/2     Running     0               3d23h   10.41.0.32    ncn-w004   <none>           <none>

cray-hbtd-bitnami-etcd-0                                          2/2     Running     0               4d      10.44.0.42    ncn-w003   <none>           <none>
cray-hbtd-bitnami-etcd-1                                          2/2     Running     1 (11h ago)     11h     10.34.0.14    ncn-w002   <none>           <none>
cray-hbtd-bitnami-etcd-2                                          2/2     Running     0               3d23h   10.41.0.33    ncn-w004   <none>           <none>

cray-hmnfd-bitnami-etcd-0                                         2/2     Running     0               4d      10.44.0.45    ncn-w003   <none>           <none>
cray-hmnfd-bitnami-etcd-1                                         2/2     Running     0               11h     10.34.0.15    ncn-w002   <none>           <none>
cray-hmnfd-bitnami-etcd-2                                         2/2     Running     0               3d23h   10.41.0.31    ncn-w004   <none>           <none>

cray-power-control-bitnami-etcd-0                                 2/2     Running     0               35h     10.36.0.24    ncn-w001   <none>           <none>
cray-power-control-bitnami-etcd-1                                 2/2     Running     0               3d23h   10.41.0.34    ncn-w004   <none>           <none>
cray-power-control-bitnami-etcd-2                                 2/2     Running     0               11h     10.34.0.19    ncn-w002   <none>           <none>

 --- PASSED --- 

**************************************************************************

=== Check if any "alarms" are set for any of the Etcd Clusters in all Namespaces. ===
=== An empty list is returned if no alarms are set ===
### cray-bss-bitnami-etcd-1 Alarms Set: ###
### cray-bss-bitnami-etcd-2 Alarms Set: ###
### cray-bss-bitnami-etcd-0 Alarms Set: ###
### cray-fas-bitnami-etcd-1 Alarms Set: ###
### cray-fas-bitnami-etcd-2 Alarms Set: ###
### cray-fas-bitnami-etcd-0 Alarms Set: ###
### cray-hbtd-bitnami-etcd-1 Alarms Set: ###
### cray-hbtd-bitnami-etcd-2 Alarms Set: ###
### cray-hbtd-bitnami-etcd-0 Alarms Set: ###
### cray-hmnfd-bitnami-etcd-1 Alarms Set: ###
### cray-hmnfd-bitnami-etcd-2 Alarms Set: ###
### cray-hmnfd-bitnami-etcd-0 Alarms Set: ###
### cray-power-control-bitnami-etcd-2 Alarms Set: ###
### cray-power-control-bitnami-etcd-0 Alarms Set: ###
### cray-power-control-bitnami-etcd-1 Alarms Set: ###
 --- PASSED --- 

**************************************************************************

=== Check the health of Etcd Cluster's database in the Services Namespace. ===
=== PASS or FAIL status returned. ===
### cray-bss-bitnami-etcd-1 Etcd Database Check: ###
PASS: OK foo fooCheck 1
### cray-bss-bitnami-etcd-2 Etcd Database Check: ###
PASS: OK foo fooCheck 1
### cray-bss-bitnami-etcd-0 Etcd Database Check: ###
PASS: OK foo fooCheck 1
### cray-fas-bitnami-etcd-1 Etcd Database Check: ###
PASS: OK foo fooCheck 1
### cray-fas-bitnami-etcd-2 Etcd Database Check: ###
PASS: OK foo fooCheck 1
### cray-fas-bitnami-etcd-0 Etcd Database Check: ###
PASS: OK foo fooCheck 1
### cray-hbtd-bitnami-etcd-1 Etcd Database Check: ###
PASS: OK foo fooCheck 1
### cray-hbtd-bitnami-etcd-2 Etcd Database Check: ###
PASS: OK foo fooCheck 1
### cray-hbtd-bitnami-etcd-0 Etcd Database Check: ###
PASS: OK foo fooCheck 1
### cray-hmnfd-bitnami-etcd-1 Etcd Database Check: ###
PASS: OK foo fooCheck 1
### cray-hmnfd-bitnami-etcd-2 Etcd Database Check: ###
PASS: OK foo fooCheck 1
### cray-hmnfd-bitnami-etcd-0 Etcd Database Check: ###
PASS: OK foo fooCheck 1
### cray-power-control-bitnami-etcd-2 Etcd Database Check: ###
PASS: OK foo fooCheck 1
### cray-power-control-bitnami-etcd-0 Etcd Database Check: ###
PASS: OK foo fooCheck 1
### cray-power-control-bitnami-etcd-1 Etcd Database Check: ###
PASS: OK foo fooCheck 1
 --- PASSED --- 

**************************************************************************

=== Verify etcd clusters have a backup in the last 24 hours. ===
=== The complete list of backups can be listed as follows:
=== % /opt/cray/platform-utils/etcd/etcd-util.sh list_backups -
Wed Nov  6 18:19:39 UTC 2024

-- cray-bss -- backups
PASS: backup found less than 24 hours old.

-- cray-fas -- backups
PASS: backup found less than 24 hours old.

-- cray-hbtd -- backups
PASS: backup found less than 24 hours old.

-- cray-hmnfd -- backups
PASS: backup found less than 24 hours old.

-- cray-power-control -- backups
PASS: backup found less than 24 hours old.
 --- PASSED --- 

**************************************************************************

=== NCN node uptimes ===
=== NCN Master nodes: ncn-m001 ncn-m002 ncn-m003 ===
=== NCN Worker nodes: ncn-w001 ncn-w002 ncn-w003 ncn-w004 ncn-w005 ===
=== NCN Storage nodes: ncn-s001 ncn-s002 ncn-s003 ===
=== date; for n in ncn-m001 ncn-m002 ncn-m003 ncn-w001 ncn-w002 ncn-w003 ncn-w004 ncn-w005 ncn-s001 ncn-s002 ncn-s003; do echo$n:; ssh $n uptime; done ===
Wed Nov  6 18:19:53 UTC 2024
ncn-m001:
 18:19:53  up 12 days  5:46,  7 users,  load average: 1.85, 1.21, 1.20
ncn-m002:
 18:19:54  up 3 days 20:45,  0 users,  load average: 2.40, 1.41, 1.16
ncn-m003:
 18:19:55  up   0:04,  1 user,  load average: 1.36, 1.11, 0.52
ncn-w001:
 18:19:58  up 1 day 11:24,  0 users,  load average: 4.29, 4.73, 4.61
ncn-w002:
 18:20:00  up  11:54,  0 users,  load average: 6.11, 5.24, 5.01
ncn-w003:
 18:20:03  up 4 days  0:41,  0 users,  load average: 6.57, 5.59, 5.08
ncn-w004:
 18:20:05  up 4 days  0:06,  0 users,  load average: 10.54, 8.43, 8.34
ncn-w005:
 18:20:05  up   5:34,  2 users,  load average: 3.08, 2.94, 3.01
ncn-s001:
 18:20:06  up 4 days  2:35,  0 users,  load average: 0.56, 0.62, 0.70
ncn-s002:
 18:20:06  up 3 days 22:06,  1 user,  load average: 1.41, 1.06, 0.79
ncn-s003:
 18:20:06  up 3 days 21:34,  0 users,  load average: 0.69, 0.56, 0.60
 --- This is an informative check. No pass or fail status to report. --- 

**************************************************************************

=== NCN master and worker node resource consumption ===
=== NCN Master nodes: ncn-m001 ncn-m002 ncn-m003 ===
=== NCN Worker nodes: ncn-w001 ncn-w002 ncn-w003 ncn-w004 ncn-w005 ===
=== date; kubectl top nodes ===
Wed Nov  6 18:20:06 UTC 2024
NAME       CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   
ncn-m001   1541m        4%     9525Mi          7%        
ncn-m002   1357m        4%     5892Mi          4%        
ncn-m003   1954m        6%     3559Mi          2%        
ncn-w001   4498m        7%     21851Mi         8%        
ncn-w002   4203m        6%     17537Mi         6%        
ncn-w003   5304m        8%     23435Mi         9%        
ncn-w004   8151m        6%     45766Mi         17%       
ncn-w005   787m         1%     4740Mi          1%        
 --- PASSED --- 

**************************************************************************

=== NCN node xnames and metal.no-wipe status ===
=== metal.no-wipe=1, expected setting - the client ===
=== already has the right partitions and a bootable ROM. ===
=== Note that before the PIT node has been rebooted into ncn-m001, ===
=== metal.no-wipe status may not available. ===
=== NCN Master nodes: ncn-m001 ncn-m002 ncn-m003 ===
=== NCN Worker nodes: ncn-w001 ncn-w002 ncn-w003 ncn-w004 ncn-w005 ===
=== NCN Storage nodes: ncn-s001 ncn-s002 ncn-s003 ===
Wed Nov  6 18:20:08 UTC 2024
ncn-m001: x3000c0s1b0n0 - metal.no-wipe=1
ncn-m002: x3000c0s2b0n0 - metal.no-wipe=1
ncn-m003: x3000c0s3b0n0 - metal.no-wipe=1
ncn-w001: x3000c0s4b0n0 - metal.no-wipe=1
ncn-w002: x3000c0s5b0n0 - metal.no-wipe=1
ncn-w003: x3000c0s6b0n0 - metal.no-wipe=1
ncn-w004: x3000c0s30b0n0 - metal.no-wipe=1
ncn-w005: x3000c0s31b0n0 - metal.no-wipe=1
ncn-s001: x3000c0s7b0n0 - metal.no-wipe=1
ncn-s002: x3000c0s8b0n0 - metal.no-wipe=1
ncn-s003: x3000c0s9b0n0 - metal.no-wipe=1
 --- PASSED ---

**************************************************************************

=== Worker ncn node pod counts ===
=== NCN Worker nodes: ncn-w001 ncn-w002 ncn-w003 ncn-w004 ncn-w005  ===
=== date; kubectl get pods -A -o wide | grep -v Completed | grep ncn-XXX | wc -l ===
Wed Nov  6 18:20:22 UTC 2024
ncn-w001: 83
ncn-w002: 60
ncn-w003: 83
ncn-w004: 115
ncn-w005: 25
 --- This is an informative check. No pass or fail status to report. --- 

**************************************************************************

=== Pods yet to reach the running state: ===
=== kubectl get pods -A -o wide | grep -Ev " (Completed|Running|(cray-dns-unbound-manager|hms-discovery)-.* (Pending|Init:0/[1-9]|PodInitializing|NotReady|Terminating)) " ===
Wed Nov  6 18:20:26 UTC 2024
NAMESPACE            NAME                                                              READY   STATUS      RESTARTS        AGE     IP            NODE       NOMINATED NODE   READINESS GATES
services             cfs-6e8dd436-412f-43ff-9467-2fa55812d992-8lfzg                    2/3     NotReady    0               3m13s   10.46.0.14    ncn-w005   <none>           <none>
 --- WARNING --- not all pods are in a 'Running' or 'Completed' state.

**************************************************************************

NCN Health Check complete. Summary of failures and warnings is printed below.

 --- Failures and Warnings--- 
WARNING: not all pods are in a 'Running' or 'Completed' state.

Two informative tests were run which checked 'NCN uptimes' and 'worker NCN node pod counts'. These results can be manually checked.

