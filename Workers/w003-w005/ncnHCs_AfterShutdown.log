=== NCN Master nodes: ncn-m001 ncn-m002 ncn-m003 ===
=== NCN Worker nodes: ncn-w001 ncn-w002 ncn-w003 ncn-w004 ncn-w005 ===
=== NCN Storage nodes: ncn-s001 ncn-s002 ncn-s003 ===

             +++++ NCN Health Checks +++++
=== Can be executed on any worker or master ncn node. ===
=== Executing on ncn-m001, Thu Nov  7 10:08:05 UTC 2024 ===
=== Active CSM version:


**************************************************************************

=== Check Kubernetes' Master and Worker Node Status. ===
=== Verify Kubernetes' Node "Ready" Status and Version. ===
Thu Nov  7 10:08:05 UTC 2024
NAME       STATUS     ROLES           AGE     VERSION    INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                              KERNEL-VERSION               CONTAINER-RUNTIME
ncn-m001   Ready      control-plane   12d     v1.24.17   10.252.1.12   <none>        SUSE Linux Enterprise Server 15 SP6   6.4.0-150600.23.17-default   containerd://1.7.21
ncn-m002   Ready      control-plane   4d12h   v1.24.17   10.252.1.11   <none>        SUSE Linux Enterprise Server 15 SP6   6.4.0-150600.23.17-default   containerd://1.7.21
ncn-m003   Ready      control-plane   4d12h   v1.24.17   10.252.1.10   <none>        SUSE Linux Enterprise Server 15 SP6   6.4.0-150600.23.17-default   containerd://1.7.21
ncn-w001   Ready      <none>          4d17h   v1.24.17   10.252.1.9    <none>        SUSE Linux Enterprise Server 15 SP6   6.4.0-150600.23.17-default   containerd://1.7.21
ncn-w002   Ready      <none>          4d16h   v1.24.17   10.252.1.8    <none>        SUSE Linux Enterprise Server 15 SP6   6.4.0-150600.23.17-default   containerd://1.7.21
ncn-w003   NotReady   <none>          4d16h   v1.24.17   10.252.1.7    <none>        SUSE Linux Enterprise Server 15 SP6   6.4.0-150600.23.17-default   containerd://1.7.21
ncn-w004   Ready      <none>          4d15h   v1.24.17   10.252.1.13   <none>        SUSE Linux Enterprise Server 15 SP6   6.4.0-150600.23.17-default   containerd://1.7.21
ncn-w005   NotReady   <none>          4d14h   v1.24.17   10.252.1.14   <none>        SUSE Linux Enterprise Server 15 SP6   6.4.0-150600.23.17-default   containerd://1.7.21
 --- FAILED --- not all nodes are "Ready" 

**************************************************************************

=== Check Ceph Health Status. ===
=== Verify "health: HEALTH_OK" Status. ===
=== At times a status of HEALTH_WARN, too few PGs per OSD, and/or large   omap objects, may be okay. ===
=== date; ssh ncn-m001 ceph -s; ===
Thu Nov  7 10:08:05 UTC 2024
  cluster:
    id:     e1ce52a6-92aa-11ef-ae72-1402ecd97c88
    health: HEALTH_OK
 
  services:
    mon: 3 daemons, quorum ncn-s001,ncn-s002,ncn-s003 (age 4d)
    mgr: ncn-s001.lwgvin(active, since 4d), standbys: ncn-s003.tzpfbg, ncn-s002.rqjssn
    mds: 2/2 daemons up, 3 standby, 1 hot standby
    osd: 24 osds: 24 up (since 4d), 24 in (since 4d)
    rgw: 3 daemons active (3 hosts, 1 zones)
 
  data:
    volumes: 2/2 healthy
    pools:   15 pools, 881 pgs
    objects: 362.53k objects, 693 GiB
    usage:   1.6 TiB used, 40 TiB / 42 TiB avail
    pgs:     881 active+clean
 
  io:
    client:   98 KiB/s rd, 3.5 MiB/s wr, 80 op/s rd, 299 op/s wr
 
 --- PASSED --- 

**************************************************************************

=== Check the Health of the Etcd Clusters in all Namespaces. ===
=== Verify a "healthy" Report for Each Etcd Pod. ===
Thu Nov  7 10:08:08 UTC 2024
### cray-bss-bitnami-etcd-1 ###
127.0.0.1:2379 is healthy: successfully committed proposal: took = 4.795929ms
### cray-bss-bitnami-etcd-2 ###
127.0.0.1:2379 is healthy: successfully committed proposal: took = 3.226494ms
### cray-fas-bitnami-etcd-1 ###
127.0.0.1:2379 is healthy: successfully committed proposal: took = 3.848938ms
### cray-fas-bitnami-etcd-2 ###
127.0.0.1:2379 is healthy: successfully committed proposal: took = 3.441255ms
### cray-hbtd-bitnami-etcd-1 ###
127.0.0.1:2379 is healthy: successfully committed proposal: took = 4.632145ms
### cray-hbtd-bitnami-etcd-2 ###
127.0.0.1:2379 is healthy: successfully committed proposal: took = 3.346229ms
### cray-hmnfd-bitnami-etcd-1 ###
127.0.0.1:2379 is healthy: successfully committed proposal: took = 3.98967ms
### cray-hmnfd-bitnami-etcd-2 ###
127.0.0.1:2379 is healthy: successfully committed proposal: took = 4.386616ms
### cray-power-control-bitnami-etcd-2 ###
127.0.0.1:2379 is healthy: successfully committed proposal: took = 4.020978ms
### cray-power-control-bitnami-etcd-0 ###
127.0.0.1:2379 is healthy: successfully committed proposal: took = 3.999808ms
### cray-power-control-bitnami-etcd-1 ###
127.0.0.1:2379 is healthy: successfully committed proposal: took = 4.793904ms
 --- PASSED --- 

**************************************************************************

=== Check the Number of Pods in Each Cluster. Verify they are Balanced. ===
=== Each cluster should contain at least three pods, but may contain more. ===
=== Ensure that no two pods in a given cluster exist on the same worker node. ===
Thu Nov  7 10:08:13 UTC 2024
cray-bss-bitnami-etcd-0                                           2/2     Terminating   0               4d16h   10.44.0.44    ncn-w003   <none>           <none>
cray-bss-bitnami-etcd-1                                           2/2     Running       0               27h     10.34.0.23    ncn-w002   <none>           <none>
cray-bss-bitnami-etcd-2                                           2/2     Running       0               2d3h    10.36.0.20    ncn-w001   <none>           <none>

cray-fas-bitnami-etcd-0                                           2/2     Terminating   0               4d16h   10.44.0.39    ncn-w003   <none>           <none>
cray-fas-bitnami-etcd-1                                           2/2     Running       0               27h     10.34.0.21    ncn-w002   <none>           <none>
cray-fas-bitnami-etcd-2                                           2/2     Running       0               4d15h   10.41.0.32    ncn-w004   <none>           <none>

cray-hbtd-bitnami-etcd-0                                          2/2     Terminating   0               4d16h   10.44.0.42    ncn-w003   <none>           <none>
cray-hbtd-bitnami-etcd-1                                          2/2     Running       1 (27h ago)     27h     10.34.0.14    ncn-w002   <none>           <none>
cray-hbtd-bitnami-etcd-2                                          2/2     Running       0               4d15h   10.41.0.33    ncn-w004   <none>           <none>

cray-hmnfd-bitnami-etcd-0                                         2/2     Terminating   0               4d16h   10.44.0.45    ncn-w003   <none>           <none>
cray-hmnfd-bitnami-etcd-1                                         2/2     Running       0               27h     10.34.0.15    ncn-w002   <none>           <none>
cray-hmnfd-bitnami-etcd-2                                         2/2     Running       0               4d15h   10.41.0.31    ncn-w004   <none>           <none>

cray-power-control-bitnami-etcd-0                                 2/2     Running       0               2d3h    10.36.0.24    ncn-w001   <none>           <none>
cray-power-control-bitnami-etcd-1                                 2/2     Running       0               4d15h   10.41.0.34    ncn-w004   <none>           <none>
cray-power-control-bitnami-etcd-2                                 2/2     Running       0               27h     10.34.0.19    ncn-w002   <none>           <none>

 --- PASSED --- 

**************************************************************************

=== Check if any "alarms" are set for any of the Etcd Clusters in all Namespaces. ===
=== An empty list is returned if no alarms are set ===
### cray-bss-bitnami-etcd-1 Alarms Set: ###
### cray-bss-bitnami-etcd-2 Alarms Set: ###
### cray-fas-bitnami-etcd-1 Alarms Set: ###
### cray-fas-bitnami-etcd-2 Alarms Set: ###
### cray-hbtd-bitnami-etcd-1 Alarms Set: ###
### cray-hbtd-bitnami-etcd-2 Alarms Set: ###
### cray-hmnfd-bitnami-etcd-1 Alarms Set: ###
### cray-hmnfd-bitnami-etcd-2 Alarms Set: ###
### cray-power-control-bitnami-etcd-2 Alarms Set: ###
### cray-power-control-bitnami-etcd-0 Alarms Set: ###
### cray-power-control-bitnami-etcd-1 Alarms Set: ###
 --- PASSED --- 

**************************************************************************

=== Check the health of Etcd Cluster's database in the Services Namespace. ===
=== PASS or FAIL status returned. ===
### cray-bss-bitnami-etcd-1 Etcd Database Check: ###
PASS: OK foo fooCheck 1
### cray-bss-bitnami-etcd-2 Etcd Database Check: ###
PASS: OK foo fooCheck 1
### cray-fas-bitnami-etcd-1 Etcd Database Check: ###
PASS: OK foo fooCheck 1
### cray-fas-bitnami-etcd-2 Etcd Database Check: ###
PASS: OK foo fooCheck 1
### cray-hbtd-bitnami-etcd-1 Etcd Database Check: ###
PASS: OK foo fooCheck 1
### cray-hbtd-bitnami-etcd-2 Etcd Database Check: ###
PASS: OK foo fooCheck 1
### cray-hmnfd-bitnami-etcd-1 Etcd Database Check: ###
PASS: OK foo fooCheck 1
### cray-hmnfd-bitnami-etcd-2 Etcd Database Check: ###
PASS: OK foo fooCheck 1
### cray-power-control-bitnami-etcd-2 Etcd Database Check: ###
PASS: OK foo fooCheck 1
### cray-power-control-bitnami-etcd-0 Etcd Database Check: ###
PASS: OK foo fooCheck 1
### cray-power-control-bitnami-etcd-1 Etcd Database Check: ###
PASS: OK foo fooCheck 1
 --- PASSED --- 

**************************************************************************

=== Verify etcd clusters have a backup in the last 24 hours. ===
=== The complete list of backups can be listed as follows:
=== % /opt/cray/platform-utils/etcd/etcd-util.sh list_backups -
Thu Nov  7 10:08:29 UTC 2024

-- cray-bss -- backups
ERROR: Expected backup because cray-bss is over 24 hours old (creationTimestamp: 2024-10-25T10:02:01Z). Expected a backup created within the last 24 hours.

-- cray-fas -- backups
ERROR: Expected backup because cray-fas is over 24 hours old (creationTimestamp: 2024-10-25T10:03:11Z). Expected a backup created within the last 24 hours.

-- cray-hbtd -- backups
ERROR: Expected backup because cray-hbtd is over 24 hours old (creationTimestamp: 2024-10-25T10:04:06Z). Expected a backup created within the last 24 hours.

-- cray-hmnfd -- backups
ERROR: Expected backup because cray-hmnfd is over 24 hours old (creationTimestamp: 2024-10-25T10:05:47Z). Expected a backup created within the last 24 hours.

-- cray-power-control -- backups
ERROR: Expected backup because cray-power-control is over 24 hours old (creationTimestamp: 2024-10-25T10:07:16Z). Expected a backup created within the last 24 hours.
 --- FAILED --- not all Etcd clusters had expected backups.

**************************************************************************

=== NCN node uptimes ===
=== NCN Master nodes: ncn-m001 ncn-m002 ncn-m003 ===
=== NCN Worker nodes: ncn-w001 ncn-w002 ncn-w003 ncn-w004 ncn-w005 ===
=== NCN Storage nodes: ncn-s001 ncn-s002 ncn-s003 ===
=== date; for n in ncn-m001 ncn-m002 ncn-m003 ncn-w001 ncn-w002 ncn-w003 ncn-w004 ncn-w005 ncn-s001 ncn-s002 ncn-s003; do echo$n:; ssh $n uptime; done ===
Thu Nov  7 10:08:44 UTC 2024
ncn-m001:
 10:08:44  up 12 days 21:34,  16 users,  load average: 1.39, 1.28, 1.33
ncn-m002:
 10:08:45  up   4:14,  0 users,  load average: 0.62, 0.68, 0.63
ncn-m003:
 10:08:46  up  15:53,  0 users,  load average: 1.05, 1.21, 1.34
ncn-w001:
 10:08:49  up 2 days  3:12,  0 users,  load average: 8.40, 7.38, 6.33
ncn-w002:
 10:08:52  up 1 day  3:43,  0 users,  load average: 1.93, 3.91, 5.19
ncn-w003:
ncn-w004:
 10:08:58  up 4 days 15:55,  0 users,  load average: 11.73, 11.78, 10.67
ncn-w005:
ncn-s001:
 10:09:01  up 4 days 18:24,  0 users,  load average: 0.94, 0.98, 0.95
ncn-s002:
 10:09:01  up 4 days 13:55,  1 user,  load average: 0.47, 0.43, 0.48
ncn-s003:
 10:09:02  up 4 days 13:23,  0 users,  load average: 0.44, 0.63, 0.69
 --- This is an informative check. No pass or fail status to report. --- 

**************************************************************************

=== NCN master and worker node resource consumption ===
=== NCN Master nodes: ncn-m001 ncn-m002 ncn-m003 ===
=== NCN Worker nodes: ncn-w001 ncn-w002 ncn-w003 ncn-w004 ncn-w005 ===
=== date; kubectl top nodes ===
Thu Nov  7 10:09:02 UTC 2024
NAME       CPU(cores)   CPU%        MEMORY(bytes)   MEMORY%     
ncn-m001   1810m        5%          9180Mi          7%          
ncn-m002   656m         2%          4563Mi          3%          
ncn-m003   745m         2%          6074Mi          4%          
ncn-w001   5047m        7%          24786Mi         9%          
ncn-w002   4461m        6%          19665Mi         7%          
ncn-w004   10292m       8%          49316Mi         19%         
ncn-w003   <unknown>    <unknown>   <unknown>       <unknown>   
ncn-w005   <unknown>    <unknown>   <unknown>       <unknown>   
 --- PASSED --- 

**************************************************************************

=== NCN node xnames and metal.no-wipe status ===
=== metal.no-wipe=1, expected setting - the client ===
=== already has the right partitions and a bootable ROM. ===
=== Note that before the PIT node has been rebooted into ncn-m001, ===
=== metal.no-wipe status may not available. ===
=== NCN Master nodes: ncn-m001 ncn-m002 ncn-m003 ===
=== NCN Worker nodes: ncn-w001 ncn-w002 ncn-w003 ncn-w004 ncn-w005 ===
=== NCN Storage nodes: ncn-s001 ncn-s002 ncn-s003 ===
Thu Nov  7 10:09:03 UTC 2024
ncn-m001: x3000c0s1b0n0 - metal.no-wipe=1
ncn-m002: x3000c0s2b0n0 - metal.no-wipe=1
ncn-m003: x3000c0s3b0n0 - metal.no-wipe=1
ncn-w001: x3000c0s4b0n0 - metal.no-wipe=1
ncn-w002: x3000c0s5b0n0 - metal.no-wipe=1
ncn-w003: Failed to obtain xname for ncn-w003
ncn-w004: x3000c0s30b0n0 - metal.no-wipe=1
ncn-w005: Failed to obtain xname for ncn-w005
ncn-s001: x3000c0s7b0n0 - metal.no-wipe=1
ncn-s002: x3000c0s8b0n0 - metal.no-wipe=1
ncn-s003: x3000c0s9b0n0 - metal.no-wipe=1
 --- PASSED ---

**************************************************************************

=== Worker ncn node pod counts ===
=== NCN Worker nodes: ncn-w001 ncn-w002 ncn-w003 ncn-w004 ncn-w005  ===
=== date; kubectl get pods -A -o wide | grep -v Completed | grep ncn-XXX | wc -l ===
Thu Nov  7 10:09:22 UTC 2024
ncn-w001: 91
ncn-w002: 76
ncn-w003: 83
ncn-w004: 139
ncn-w005: 31
 --- This is an informative check. No pass or fail status to report. --- 

**************************************************************************

=== Pods yet to reach the running state: ===
=== kubectl get pods -A -o wide | grep -Ev " (Completed|Running|(cray-dns-unbound-manager|hms-discovery)-.* (Pending|Init:0/[1-9]|PodInitializing|NotReady|Terminating)) " ===
Thu Nov  7 10:09:26 UTC 2024
NAMESPACE            NAME                                                              READY   STATUS        RESTARTS         AGE     IP            NODE       NOMINATED NODE   READINESS GATES
argo                 cray-nls-argo-workflows-server-564cd584d9-jfpnf                   2/2     Terminating   0                4d16h   10.44.0.12    ncn-w003   <none>           <none>
ceph-cephfs          cray-ceph-csi-cephfs-provisioner-788df75f76-zhs7q                 6/6     Terminating   3 (4h58m ago)    4d16h   10.44.0.21    ncn-w003   <none>           <none>
ceph-rbd             cray-ceph-csi-rbd-provisioner-6974c7d78b-q8xlq                    7/7     Terminating   11 (4h58m ago)   4d16h   10.44.0.50    ncn-w003   <none>           <none>
cert-manager         cray-certmanager-cert-manager-7d8995464d-rxhvk                    2/2     Terminating   0                4d16h   10.44.0.22    ncn-w003   <none>           <none>
cert-manager         cray-certmanager-cert-manager-cainjector-6c7845f6dc-kvpl2         2/2     Terminating   1 (4h58m ago)    4d16h   10.44.0.23    ncn-w003   <none>           <none>
cert-manager         cray-certmanager-cert-manager-webhook-57dbbff888-zx55x            2/2     Terminating   3 (4d16h ago)    4d16h   10.44.0.47    ncn-w003   <none>           <none>
istio-system         istio-ingressgateway-94df8f964-kcmdd                              1/1     Terminating   0                4d16h   10.44.0.29    ncn-w003   <none>           <none>
istio-system         istio-ingressgateway-customer-admin-ffc69c5f-5fksl                1/1     Terminating   0                4d16h   10.44.0.15    ncn-w003   <none>           <none>
istio-system         istio-ingressgateway-customer-user-77fbcc86d7-475jz               1/1     Terminating   0                4d16h   10.44.0.30    ncn-w003   <none>           <none>
istio-system         istio-ingressgateway-hmn-785b96ddb8-7t795                         1/1     Terminating   0                4d16h   10.44.0.27    ncn-w003   <none>           <none>
istio-system         istiod-6bb7bc7fb9-bggqx                                           1/1     Terminating   0                4d15h   10.44.0.33    ncn-w003   <none>           <none>
istio-system         kiali-7cf8968ff4-lbmn4                                            1/1     Terminating   0                4d16h   10.44.0.52    ncn-w003   <none>           <none>
kube-system          coredns-59cc8dbcc8-285sw                                          1/1     Terminating   0                4h52m   10.46.0.14    ncn-w005   <none>           <none>
kube-system          kube-etcdbackup-28849540-dxz7w                                    0/1     Terminating   0                29m     <none>        ncn-w005   <none>           <none>
kube-system          metrics-server-545fcf988f-r4s2t                                   1/1     Terminating   0                4d16h   10.44.0.53    ncn-w003   <none>           <none>
kyverno              kyverno-admission-controller-5fb8868644-q8hnb                     1/1     Terminating   0                4d16h   10.44.0.26    ncn-w003   <none>           <none>
kyverno              kyverno-background-controller-5bb9c4566d-wspfz                    1/1     Terminating   0                4d16h   10.44.0.28    ncn-w003   <none>           <none>
kyverno              kyverno-cleanup-admission-reports-28849540-k7jfr                  0/1     Terminating   0                29m     <none>        ncn-w005   <none>           <none>
kyverno              kyverno-cleanup-cluster-admission-reports-28849540-hq45t          0/1     Terminating   0                29m     <none>        ncn-w005   <none>           <none>
operators            cray-kiali-kiali-operator-66645594fc-p5prx                        1/1     Terminating   0                4d16h   10.44.0.20    ncn-w003   <none>           <none>
pki-operator         trustedcerts-operator-f546977cb-hvbtx                             2/2     Terminating   3 (4d16h ago)    4d16h   10.44.0.46    ncn-w003   <none>           <none>
services             cfs-ara-postgres-1                                                3/3     Terminating   0                4d12h   10.44.0.19    ncn-w003   <none>           <none>
services             cfs-trust-68fc9fcdb4-sp44l                                        2/2     Terminating   2 (4d16h ago)    4d16h   10.44.0.24    ncn-w003   <none>           <none>
services             cray-bos-75bb8ddbf9-g94cv                                         2/2     Terminating   0                4d16h   10.44.0.14    ncn-w003   <none>           <none>
services             cray-bos-db-788bcbc84c-6xkwc                                      2/2     Terminating   0                4d16h   10.44.0.62    ncn-w003   <none>           <none>
services             cray-bss-79785c6d89-kq8vf                                         2/2     Terminating   0                4d16h   10.44.0.32    ncn-w003   <none>           <none>
services             cray-bss-bitnami-etcd-0                                           2/2     Terminating   0                4d16h   10.44.0.44    ncn-w003   <none>           <none>
services             cray-capmc-6c5d69f685-4m44m                                       2/2     Terminating   0                4d16h   10.44.0.60    ncn-w003   <none>           <none>
services             cray-console-node-0                                               3/3     Terminating   1 (4h58m ago)    4d16h   10.44.0.72    ncn-w003   <none>           <none>
services             cray-dhcp-kea-56c5bb5789-96vj5                                    3/3     Terminating   4 (4h54m ago)    4d16h   10.44.0.59    ncn-w003   <none>           <none>
services             cray-dhcp-kea-helper-28849539-f9mj7                               1/2     Terminating   0                30m     10.46.0.15    ncn-w005   <none>           <none>
services             cray-dhcp-kea-helper-28849569-jbrh7                               1/2     NotReady      0                26s     10.41.0.129   ncn-w004   <none>           <none>
services             cray-dns-powerdns-postgres-2                                      3/3     Terminating   0                4d12h   10.44.0.68    ncn-w003   <none>           <none>
services             cray-dns-unbound-79b56d4645-ldnvn                                 3/3     Terminating   0                4d16h   10.44.0.31    ncn-w003   <none>           <none>
services             cray-fas-bitnami-etcd-0                                           2/2     Terminating   0                4d16h   10.44.0.39    ncn-w003   <none>           <none>
services             cray-hbtd-55fb95cc7b-6srfv                                        2/2     Terminating   0                4d16h   10.44.0.35    ncn-w003   <none>           <none>
services             cray-hbtd-bitnami-etcd-0                                          2/2     Terminating   0                4d16h   10.44.0.42    ncn-w003   <none>           <none>
services             cray-hmnfd-747b78f958-n6m76                                       2/2     Terminating   0                4d16h   10.44.0.36    ncn-w003   <none>           <none>
services             cray-hmnfd-bitnami-etcd-0                                         2/2     Terminating   0                4d16h   10.44.0.45    ncn-w003   <none>           <none>
services             cray-hms-hmcollector-ingress-7c5bb55c74-hd9rz                     2/2     Terminating   0                4d16h   10.44.0.61    ncn-w003   <none>           <none>
services             cray-keycloak-0                                                   2/2     Terminating   0                4d16h   10.44.0.64    ncn-w003   <none>           <none>
services             cray-postgres-operator-596b6cfb9c-lcxd2                           2/2     Terminating   0                4d16h   10.44.0.40    ncn-w003   <none>           <none>
services             cray-power-control-6f9869d68-7kgz4                                2/2     Terminating   1 (4d16h ago)    4d16h   10.44.0.34    ncn-w003   <none>           <none>
services             cray-scsd-b744ccc54-sdz25                                         2/2     Terminating   0                4d16h   10.44.0.55    ncn-w003   <none>           <none>
services             cray-shared-kafka-kafka-2                                         1/1     Terminating   1 (4d16h ago)    4d16h   10.44.0.38    ncn-w003   <none>           <none>
services             cray-shared-kafka-zookeeper-1                                     1/1     Terminating   0                4d16h   10.44.0.43    ncn-w003   <none>           <none>
services             cray-sls-9d467b6d6-ns8pv                                          2/2     Terminating   0                4d16h   10.44.0.18    ncn-w003   <none>           <none>
services             cray-sls-postgres-2                                               3/3     Terminating   0                4d12h   10.44.0.67    ncn-w003   <none>           <none>
services             cray-tftp-5d546777c9-4f67h                                        1/1     Terminating   0                4d16h   10.44.0.65    ncn-w003   <none>           <none>
services             cray-vpa-recommender-86867b7bf6-p65sg                             2/2     Terminating   0                4d16h   10.44.0.54    ncn-w003   <none>           <none>
services             csm-ssh-keys-7ccc4f5494-nc44c                                     2/2     Terminating   1 (4d16h ago)    4d16h   10.44.0.49    ncn-w003   <none>           <none>
services             etcd-backup-restore-75bb5c4694-4kgp5                              1/1     Terminating   0                4d16h   10.44.0.51    ncn-w003   <none>           <none>
services             gitea-vcs-856dfb98cf-v52ws                                        2/2     Terminating   0                4d16h   10.44.0.66    ncn-w003   <none>           <none>
services             gitea-vcs-postgres-2                                              3/3     Terminating   0                21h     10.44.0.70    ncn-w003   <none>           <none>
services             keycloak-postgres-0                                               3/3     Terminating   0                21h     10.44.0.69    ncn-w003   <none>           <none>
services             sonar-sync-28849539-5k5l7                                         1/1     Terminating   0                30m     10.46.0.16    ncn-w005   <none>           <none>
services             sshot-fm-recover-28849539-k4rrz                                   1/2     Terminating   0                30m     10.46.0.17    ncn-w005   <none>           <none>
services             sshot-fm-recover-28849569-cwcqx                                   1/2     NotReady      0                26s     10.41.0.105   ncn-w004   <none>           <none>
spire                cray-spire-jwks-847fc745b8-n925t                                  3/3     Terminating   0                4d16h   10.44.0.37    ncn-w003   <none>           <none>
spire                cray-spire-postgres-2                                             3/3     Terminating   0                4d12h   10.44.0.17    ncn-w003   <none>           <none>
spire                cray-spire-server-0                                               2/2     Terminating   0                4d16h   10.44.0.63    ncn-w003   <none>           <none>
spire                spire-jwks-6fbb5864db-4cxg2                                       3/3     Terminating   0                4d16h   10.44.0.41    ncn-w003   <none>           <none>
spire                spire-postgres-1                                                  3/3     Terminating   0                4d12h   10.44.0.13    ncn-w003   <none>           <none>
spire                spire-postgres-pooler-6c97d87dd5-nt7xb                            2/2     Terminating   0                4d16h   10.44.0.25    ncn-w003   <none>           <none>
sysmgmt-health       cray-sysmgmt-health-prometheus-snmp-exporter-5cb4c85bb8-j8pdq     1/1     Terminating   0                4d16h   10.44.0.58    ncn-w003   <none>           <none>
sysmgmt-health       vmagent-vms-1-8c8799964-jk4pr                                     2/2     Terminating   0                4d16h   10.44.0.48    ncn-w003   <none>           <none>
sysmgmt-health       vmalert-vms-5fdd8f54dd-9j82s                                      2/2     Terminating   0                4d16h   10.44.0.57    ncn-w003   <none>           <none>
sysmgmt-health       vminsert-vms-cd9d4f894-5wzts                                      1/1     Terminating   0                4d16h   10.44.0.56    ncn-w003   <none>           <none>
tapms-operator       cray-tapms-operator-57c6cfccf7-czq7d                              2/2     Terminating   12 (4h58m ago)   4d16h   10.44.0.73    ncn-w003   <none>           <none>
vault                cray-vault-0                                                      5/5     Terminating   1 (4d16h ago)    4d16h   10.44.0.71    ncn-w003   <none>           <none>
 --- WARNING --- not all pods are in a 'Running' or 'Completed' state.

**************************************************************************

NCN Health Check complete. Summary of failures and warnings is printed below.

 --- Failures and Warnings--- 
FAIL: not all nodes are "Ready".
FAIL: not all Etcd clusters had expected backups.
WARNING: not all pods are in a 'Running' or 'Completed' state.

Two informative tests were run which checked 'NCN uptimes' and 'worker NCN node pod counts'. These results can be manually checked.

